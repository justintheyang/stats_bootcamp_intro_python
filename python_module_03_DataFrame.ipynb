{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fundamental Python - Data wrangling and analysis using Pandas\n",
    "\n",
    "This tutorial is based on Russ Poldrack's [PythonForRUsers](https://github.com/poldrack/PythonForRUsers) tutorials and is adpated to a Python-only tutorial by Shao-Fang Wang (2020) and Ben Prystawski (2022).  \n",
    "\n",
    "Many people have contributed to developing and revising the R tutorial material (which is what this Python tutorial is based on) over the years: \n",
    "Anna Khazenzon, Cayce Hook, Paul Thibodeau, Mike Frank, Benoit Monin, Ewart Thomas, Michael Waskom, Steph Gagnon, Dan Birman, Natalia Velez, Kara Weisman, Andrew Lampinen, Joshua Morris, Yochai Shavit, Jackie Schwartz, Arielle Keller, and Leili Mortazavi.    \n",
    "\n",
    "--- \n",
    "In this notebook we will explore how to use the Pandas library to work with data. \n",
    "First we need to import some necessary libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro to Numpy\n",
    "\n",
    "Numpy is a Python library that lets you work with vectors, matrices, and higher-dimensional tensors. The main object in numpy is called an array and you can define one with `np.array`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# arrays can be one-dimensional\n",
    "arr1 = np.array([1, 2, 3, 4])\n",
    "# two-dimensional\n",
    "arr2 = np.array([[1, 2, 3], [4, 5, 6], [7,8,9]])\n",
    "# and three (or more!) dimensional\n",
    "arr3 = np.array([[[1, 2], [3, 4]], [[5, 6], [7, 8]]])\n",
    "\n",
    "# you can use .shape to look at the shape of an array\n",
    "print(arr1.shape)\n",
    "print(arr2.shape)\n",
    "print(arr3.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Numpy supports most linear algebra you might want to do. Here's a sample of a few functions, but if you want to do something not covered here, you can probably find a function that does it with a quick google search or look through the [numpy documentation](https://numpy.org/doc/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec1 = np.array([2, 6, 8])\n",
    "vec2 = np.array([1, 2, 1])\n",
    "    \n",
    "# You can add two arrays of the same shape\n",
    "print(vec1 + vec2)\n",
    "\n",
    "# * does element-wise multiplication\n",
    "print(vec1 * vec2)\n",
    "\n",
    "# you can also multiply an array by a constant\n",
    "print(2 * vec1)\n",
    "\n",
    "# you can do dot products with .dot\n",
    "print(vec1.dot(vec2))\n",
    "\n",
    "# you can also do matrix-vector multiplication with .dot (don't ask me why)\n",
    "vec3 = np.array([1, 1])\n",
    "mat1 = np.array([[2, 8], [0, -1]])\n",
    "print(mat1.dot(vec3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data types in Pandas\n",
    "\n",
    "Pandas is one of the best options for working with tabular data in Python. There are two main types of data structures in Pandas that we will need to use: Series, and DataFrames. The DataFrame is the way Pandas represents a table, and Series is the data structure Pandas uses to represent a column.\n",
    "\n",
    "Pandas is built on top of the Numpy library, which in practice means that most of the methods defined for Numpy Arrays apply to Pandas Series/DataFrames.\n",
    "\n",
    "What makes Pandas so attractive is the powerful interface to access individual records of the table, proper handling of missing values, and relational-databases operations between DataFrames."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataFrame\n",
    "\n",
    "DataFrame is a 2-dimensional labeled data structure with columns of potentially different types. You can think of it like a spreadsheet. It is the most commonly used pandas object. \n",
    "We will spend a good bit of time on DataFrames because of their importance.  We will move to working with real data shortly, but we will start with a simple example. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a DataFrame in Python.  There are various ways to get data into a dataframe - in this case we will specify them as a dictionary to the ```pandas.DataFrame()``` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([1, 2, 3, 4, 5, 6, 7, 8, 9])\n",
    "y = x * 2 + 3\n",
    "group = ['1','1','2','2','1','1','2','2','2']\n",
    "\n",
    "df = pd.DataFrame({'x': x, 'y': y, 'group': group})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we would like to check out the data. Here are some useful ways to view DataFrame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check out all the column names\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summarize the variables in the DataFrame\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check the dimension of the dataframe (one of the common ways to do sanity checks)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also view the top or the bottom of the DataFrame by using `.head()` and `.tail()`. By default, it will display the first/last 5 rows. You can put number of rows you would like to view in the parentheses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#useful way to view the top (first 5 rows) of the dataframe\n",
    "df.head()\n",
    "\n",
    "#try df.head(11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#useful way to view the end (last 5 rows) of the dataframe\n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# useful way to see a random n rows of the dataframe\n",
    "df.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Indexing columns\n",
    "\n",
    "There are several different ways to access specific elements of a Pandas DataFrame.  First, let's look at accessing the columns. There are two ways to access a column by name: using the dot notation or putting its name in brackets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# access one of the variables in the DataFrame\n",
    "# note that this becomes a pandas Series\n",
    "print(df.y)\n",
    "print(df['y'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also select multiple columns by passing in an iterable (e.g. list or tuple) of column names in the brackets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df[[\"x\", \"y\"]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The columns can also be accessed based on their numeric position. This is done using the ```.iloc``` operator to the DataFrame. The first dimension in the iloc argument refers to the row, and the second to the column.  Thus, if we wanted to access the first three elements in the second columns, we would use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.iloc[:3, 1]  # columns are indexed from zero, so 1 refers to the second column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, you can see that we use `:` on its own to mean all columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.iloc[2, :]  # columns are indexed from zero, so 1 refers to the second column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Indexing rows\n",
    "\n",
    "There are two different ways to access a row in a Pandas DataFrame. \n",
    "\n",
    "The first you have already seen in the previous section, using the ```.iloc``` operator to index them numerically based on their position in the array.  \n",
    "\n",
    "The second relies upon the DataFrame's index, using the ```.loc``` operator.  Let's say that we want to extract the third row, which in this case has an index value of 2.  We could do that as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.loc[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Often we would like a more descriptive index.  For example, let's say that our nine rows refer to nine different cities in California.  We can set the index using the ```index``` operator:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.index = ['Los Angeles', 'Santa Barbara', 'Sacramento', 'Bakersfield', \n",
    "            'San Francisco','Santa Cruz','Santa Barbara','Oakland','San Diego']\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, if we want to access the data for Sacremento, we can do that using the ```.loc``` operator:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc['Sacramento']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select multiple rows\n",
    "df.loc[[\"Sacramento\", \"Oakland\", \"Santa Cruz\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Apply your knowledge\n",
    "#Can you select all columns for Bakersfield using .iloc?\n",
    "\n",
    "\n",
    "\n",
    "#Can you select all columns for Bakersfield using .loc?\n",
    "\n",
    "\n",
    "\n",
    "#Can you select the column group from the DataFrame?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Wrangling\n",
    "#### Filtering values in DataFrame\n",
    "There are many ways to filter a DataFrame. Here are a few examples for different situations.  \n",
    "1. We can filter rows of a DataFrame by values in a certain column. This type of filteirng can be done by using equality operator (`==`) and comparison operators (`>`,`<`). We first identify rows that are True to the condition and pass this information within the DataFrame to select the rows corresponding to True."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Which values in column x are larger than 5?\n",
    "df['x']>5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Filter out these rows from the DataFrame\n",
    "new_filter_data = df[df['x']>5]\n",
    "new_filter_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. We can also use `.isin()` function to check whether each elemnt in a certain column is contained in values. Every element in the specified column will be checked to see whether it is in the list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check whether elements in column x contains 6 or 7\n",
    "df['x'].isin([6,7])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we can use the Boolean output to filter the DataFrame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['x'].isin([6, 7])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also filter based on a conjunction or disjunction of criteria with the bitwise and (`&`), or (`|`), and not (`~`) operators. These let us combine different conditions for filtering. For example, we can select only the rows where x is in \\[6,7\\] and y is less than 16. When doing this, make sure to put each condition in parentheses to avoid them being evaluated weirdly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(df['x'].isin([6,7])) & (df['y'] < 16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[(df['x'].isin([6,7])) & (df['y'] < 16)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. We can also subset the DataFrame rows or columns according to the specified index labels using `filter`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#like: keep labels from axis for which “like in label == True”.\n",
    "#axis: the axis to filter on -- 0 or ‘index’, 1 or ‘columns’, None\n",
    "df.filter(like = 'San', axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Apply your knoowledge \n",
    "#Please select the rows that x >5 and in group 1\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adding columns/rows to a DataFrame\n",
    "We can use the `pd.concat()` function to combine the rows of two DataFrames, returning a new DataFrame object. Columns in one DataFrame but not the other are populated with NaN value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_row = pd.Series({'x':1, 'y':5,'group':'1'})\n",
    "new_row.name = 'San Jose'\n",
    "df = pd.concat((df, new_row))\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To add a new column to a DataFrame, we can declare a new list or a new pandas.Series (see Series in Pandas section) as a column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['new']=df['y']*2+1\n",
    "print(type(df['y']*2+1))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Group By: split-apply-combine\n",
    "We can use `groupby` operation to \n",
    "1. split the data into groups based on some cirteria: pass the name of the column you want to group on in `groupby()`\n",
    "2. apply a function to each group independently: `mean()`, `count()`, `median()`, etc\n",
    "3. combine the results into data structure. \n",
    "\n",
    "This can be used to group large amounts of data and compute operations on these groups. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#separate our DataFrame into groups according to the column group\n",
    "#calculate mean values for each group, each column\n",
    "mean_group = df.groupby('group').mean()\n",
    "print(mean_group)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Apply your knowledge\n",
    "#Calculate the count of each group\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use `[column name]` after `groupby()` to specify the columns on which we would like to perform the function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_groupx = df.groupby('group')['x'].mean()\n",
    "print(mean_groupx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To perform multiple basic functions, we can use `dataframe.aggregate()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby('group')['x'].aggregate(['mean','min'])\n",
    "#df.groupby('group')['x'].aggregate(['mean']) is the same as df.groupby('group')['x'].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, functions that are available for `aggregate()` are limited. Here are the 13 aggregating functions available in pandas:\n",
    "* mean(): Compute mean of groups\n",
    "* sum(): Compute sum of group values\n",
    "* size(): Compute group sizes\n",
    "* count(): Compute count of group\n",
    "* std(): Standard deviation of groups\n",
    "* var(): Compute variance of groups\n",
    "* sem(): Standard error of the mean of groups\n",
    "* describe(): Generates descriptive statistics\n",
    "* first(): Compute first of group values\n",
    "* last(): Compute last of group values\n",
    "* nth() : Take nth value, or a subset if n is a list\n",
    "* min(): Compute min of group values\n",
    "* max(): Compute max of group values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To be more flexible, we can use `apply` function which applyes a function along an axis of the DataFrame.\n",
    "\n",
    "Objects passed to the function are Series objects whose index is either the DataFrame’s index (axis=0) or the DataFrame’s columns (axis=1). By default (result_type=None), the final return type is inferred from the return type of the applied function. Otherwise, it depends on the result_type argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#let's reset the index \n",
    "df = df.reset_index().rename(columns={'index': 'city'})\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define a function\n",
    "def subfunc(input_df):\n",
    "    input_df['max_xvalues'] = max(input_df.x)\n",
    "    input_df['min_xvalues'] = min(input_df.x)\n",
    "    input_df['meanx_sub_meany'] = np.mean(input_df.x)-np.mean(input_df.y)\n",
    "    return input_df\n",
    "#apply the function to each group\n",
    "df.groupby('group').apply(subfunc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`apply` is especially powerful when combined with python's `lambda` notation. `lambda` lets you define single-line functions on the fly. You can use it as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.apply(lambda row: row[\"x\"] - row[\"y\"] if row[\"group\"] == 1 else row[\"x\"] - row[\"y\"], axis=1) # axis=1 applies the function row-wise rather than column-wise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading and working with data\n",
    "\n",
    "Now, let's read a real dataset from disk and show how we would work with it using Pandas.\n",
    "\n",
    "We will use a real dataset from [Eisenberg et al, 2019](nature.com/articles/s41467-019-10301-1), which contains data for 522 individuals on 192 variables derived from a set of psychological tasks.  Let's say that we want to know whether the stop signal reaction time (SSRT - measured across several different tasks) is related to self-reported impulsivity (as measured using a number of different surveys).  \n",
    "\n",
    "First we load the data.  The subject codes are contained in the first column, and we tell pandas to use those codes as the index for the DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('./data/meaningful_variables_clean.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have loaded the data, let's take a look at the DataFrame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Apply your knowledge\n",
    "#how to check the first few rows of the DataFrame? (take a look at the data)?\n",
    "\n",
    "\n",
    "#how to see column names of a DataFrame?\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To perform our analysis, we need to select the variables that we will use for the analysis. First, let's find the SSRT variables.  To do this, we will loop through all of the variable names and find the ones that include \"ssrt\". "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's loop through all of the column names and save the ones that have 'ssrt' in their name.  We will learn more about processing strings in a later section; here we will introduce the ```.find()``` operator that is present for strings.  This operator tells us the location of a particular string within another string, or returns -1 if the string is not present.  For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = 'This is a string'\n",
    "print(a.find('is'))\n",
    "print(a.find('number'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's find all the column names that contain \".SSRT\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#First find SSRT variables\n",
    "#Use what you have learned to check all column names to see which ones contain '.SSRT' \n",
    "#and save the names to ssrt_variables\n",
    "ssrt_variables = []  # empty list to save names\n",
    "#hint: s.append(x): appends x to the end of the sequence\n",
    "#advance: can you use one line of code to achieve th goal (hint:list comprehension)?\n",
    "#advance: can you use .filter function?\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far we have found the variables of interest for SSRT.  However, if we want to find other variables of interest, we need to repeate the same code, which is in bad form.  What we should do instead is find a way to loop through the variables we are interested in, so that if we decide that we want to add more then we can do that easily later.  First, we should set up a dictionary that contains all of the search strings for each data type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_strings = {\n",
    "    'SSRT': '.SSRT',\n",
    "    'BIS-11': 'bis11',\n",
    "    'UPPS-P': 'upps',\n",
    "    'Dickman': 'dickman'\n",
    "}\n",
    "print(search_strings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's adapt the code above to loop through the different variables.  Tip: Putting a dictionary as the sequence in a for loop causes it to loop over all of the keys in the dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "variable_names = {}\n",
    "variables_to_keep=[] # collect all the variable names to select them from the DataFrame\n",
    "for ss in search_strings: # loop through the keys in the dictionary\n",
    "    variable_names[ss] = []  # create empty list to store matching names for this string\n",
    "    for c in data.columns: # loop through the column names\n",
    "        if c.find(search_strings[ss]) > -1:\n",
    "            variable_names[ss].append(c)\n",
    "            variables_to_keep.append(c) # collect all column names that meet our criteria\n",
    "\n",
    "print(variable_names)\n",
    "print(variables_to_keep)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have found all of the relevant variables, let's create a new DataFrame that only includes those variables.  We have already created a list with all of the matching variables in the above for loops (`variables_to_keep`). Now we can use the list to filter the DataFrame for these columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_selected = data[variables_to_keep]\n",
    "print(data_selected.columns)\n",
    "print(data_selected.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's clean up the data. If we want to know which cells contain na values, we can use `.isna()` to identify all the na values in a DataFrame. We can also use `.fillna` to replace the na values with other values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data_selected.isna().tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data_selected.fillna(0).head()) # this is replacing NA with 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can remove all of the missing data, using the ```.dropna()``` operator.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_selected = data[variables_to_keep]\n",
    "data_selected = data_selected.dropna()\n",
    "print(data_selected.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The advantage of using a dictionary is that we can flexibly select columns based on the column name groups. For example, now we can compute the average of all SSRT variables and the average of all of the impulsivity variables.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_selected['mean_SSRT'] = data_selected[variable_names['SSRT']].mean(axis=1)\n",
    "data_selected.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a list containing all of the variables except those for SSRT\n",
    "# using set operations\n",
    "\n",
    "impulsivity_variables = set(variables_to_keep).difference(variable_names['SSRT'])\n",
    "impulsivity_variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we need to turn the set back into a list so it can be used as an index\n",
    "data_selected['mean_impulsivity'] = data_selected[list(impulsivity_variables)].mean(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can compute the correlation between those two mean variables of interest, using the built-in correlation method ```.corr()```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_selected[['mean_impulsivity', 'mean_SSRT']].corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have seen how to select only certain columns using dictionaries. However, if we only want to select variable names and do not need to access the variable names in the future, we can use `filter`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_use_filter_to_select = data.filter(regex= '\\.SSRT|bis11|upps|dickman',axis = 1)\n",
    "data_use_filter_to_select.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above code uses a *regular expression*, which is a super powerful way of selecting, filtering, and extracting certain parts of text. It's probably worth learning at least some of the regex syntax. You can do so via a fun crossword game here: https://regexcrossword.com/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Joining datasets\n",
    "\n",
    "Often we will want to combine data that are stored in multiple files. Pandas provides tools for merging datasets using a common set of indices, using the ```.merge()``` operator.  Let's say that we want to combine our data above with some demographic data, so that we can ask whether impulsivity and SSRT are related to ever having been arrested in one's life.  These data are stored in a separate file (demographics.csv).  First we load the file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Apply your knowledge\n",
    "#read in demographics.csv as a DataFrame called demogdata\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a variable called \"ArrestedChargedLifeCount\" that contains the self-reported number of times that a person has been arrested in their life.  Let's create a new variable that is True if the person has ever been arrested:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Apply your knowledge\n",
    "#column \"ArrestedChargedLifeCount\" contains the number of times \n",
    "#that a person has been arrested in his/her life. Use this information to create a new column called \"EverArrested\" \n",
    "#that is True if the person has ever been arrested.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: When you are referring to the value of a variable within a DataFrame, you can use either ```dataframe.VariableName``` or ```dataframe['VariableName']```.  However, when you are adding a new variable to a DataFrame, you must use the latter syntax.\n",
    "\n",
    "Now let's join the new variable with the data from above.  We want to only include cases that are present in both datasets, so we use what is called an \"inner join\" (see [the pandas.DataFrame.join help](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.join.html) for more on this).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_selected_with_arrest = data_selected[['mean_impulsivity', 'mean_SSRT']].join(demogdata['EverArrested'], how = 'inner')\n",
    "data_selected_with_arrest.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "inner: form intersection of calling frame’s index (or column if on is specified) with other’s index, preserving the order of the calling’s one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grouping variables using groupby()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we want to summarize the data by the different groups: Arrested vs. never arrested. Here we can apply the method ```.groupby()``` we have learned previously. First let's group the data and compute the mean for each group on each variable, and then compute normal confidence intervals for each mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Apply your knowledge\n",
    "# compute means for each group/variable based on \"EverAressted\"\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute standard errors\n",
    "stderr_by_group = data_selected_with_arrest.groupby('EverArrested').std()/np.sqrt(data_selected_with_arrest.shape[0])\n",
    "print(stderr_by_group)\n",
    "\n",
    "# compute normal confidence intervals\n",
    "upper_ci = mean_by_group + stderr_by_group * 1.96\n",
    "upper_ci.columns = [i + '_upperCI' for i in upper_ci.columns] # fix names\n",
    "\n",
    "lower_ci = mean_by_group - stderr_by_group * 1.96\n",
    "lower_ci.columns = [i + '_lowerCI' for i in lower_ci.columns] # fix names\n",
    "\n",
    "# add back into a single DataFrame\n",
    "results = pd.concat([mean_by_group, upper_ci, lower_ci], axis = 1) #, ))\n",
    "# reorder the columns\n",
    "result_columns = results.columns.tolist()\n",
    "result_columns.sort()\n",
    "results = results[result_columns]\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Eyeballing these results, it appears that there is a difference in impulsivity between the groups.  We will return to this later when we move on to statistical inference.  Before we leave, let's save the data to a csv file so that we can read it back in when we return to this later.  This is easy using the ```.to_csv()``` method with the DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_selected_with_arrest.to_csv('./data/arrest_ssrt_impulsivity.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Series in Pandas\n",
    "\n",
    "A Series is a one-dimensional data structure similar to an array or a list.  The main difference between a Pandas Series and a list is that the Series contains some additional indexing information.  To see how this works, let's generate some numbers and put them in a Series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(12345)  # fix the random seed for reproducibility\n",
    "\n",
    "s = pd.Series(np.random.randn(5))\n",
    "print(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The index for this Series by default is a series of integers starting at zero.  However, we can also specify an index explicity. Let's say that our 5 numbers come from 5 different people, and we want to use their names as the index:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_indexed = pd.Series(np.random.randn(5),\n",
    "                  index=['Lisa', 'Sue', 'Karen', 'Lucy', 'Helen'])\n",
    "print(s_indexed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also access the index directly using the ```.index``` element of the Series:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_indexed.index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we can set the index using that element as well:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s.index = ['Lisa', 'Sue', 'Karen', 'Lucy', 'Helen']\n",
    "print(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A nice feature is that we can then access the data using the index:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# access a single element\n",
    "print(s['Helen'])\n",
    "\n",
    "# access a range of elements\n",
    "print(s['Karen':'Helen'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also index based on the values of the series --- for example, we can find all entries with values less than zero:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(s[s<0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This works because ```s < 0``` returns a Boolean series, and using a Boolean series as a index to another Series will return all of the places where the Boolean series is true:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(s < 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes you may want to extract the values from a Series back into a Numpy array. You can do this using the ```.values``` element of the Series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can sort the Series according to the data values using the ```.sort_values()``` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_sorted = s.sort_values()\n",
    "print(s_sorted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's say that you wanted to only take the top three of the sorted list. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_sorted[-3:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What if you aren't sure whether an item is in the Series?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Arielle' in s)\n",
    "print('Lucy' in s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mathematical operations can be done using scalars and functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s/10\n",
    "np.square(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s2 = pd.Series(np.random.randn(3),\n",
    "                  index=['John', 'Jack', 'Will'])\n",
    "s.append(s2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Apply your knowledge\n",
    "#You are looking for a new phone. You obtain a list of iphones and their prices and a list of Samsung phones and their prices.\n",
    "price1 = pd.Series([799,650,999],\n",
    "                  index=['iphone11', 'iphonese', 'iphone11pro'])\n",
    "price2 = pd.Series([899,780,1000,800],\n",
    "                  index=['GalaxyS9', 'GalaxyS8', 'GalaxyS9plus','GalaxyS8plus'])\n",
    "\n",
    "##Combine these two series and sort the values from low to high\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "##You do not want any phones that are more expensive than $850. Which phones are available now?\n",
    "\n",
    "\n",
    "\n",
    "##You decided not to consider GalaxyS8. Can you drop GalaxyS8 from the series by its index? (google)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resources\n",
    "There are some great tutorials on Pandas linked [here](http://www.data-analysis-in-python.org/3_pandas.html).\n",
    "https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.filter.html"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
